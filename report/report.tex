\documentclass{article}

\usepackage[margin=3cm]{geometry}
\usepackage{setspace}

\usepackage[british]{babel}
\usepackage{csquotes}
\usepackage{hyperref}

\usepackage{titling}
\setlength{\droptitle}{-2cm}

\usepackage[backend=biber,style=alphabetic]{biblatex}
\addbibresource{\jobname.bib}

\usepackage{acronym}
\acrodef{ACT}{auto-correlation time}
\acrodef{ESS}{effective sample size}
\acrodef{HMC}{Hamiltonian Monte Carlo}
\acrodef{MCMC}{Markov chain Monte Carlo}
\acrodef{ODE}{ordinary differential equation}

\usepackage{mathtools}
\newcommand{\dd}{\, \mathrm{d}}
\renewcommand{\vec}[1]{\ensuremath{\boldsymbol{\mathbf{#1}}}}
\newcommand{\mat}[1]{\ensuremath{\boldsymbol{\mathbf{#1}}}}
\newcommand{\op}[1]{\ensuremath{\boldsymbol{\mathbf{#1}}}}
\newcommand{\norm}[1]{\ensuremath{\mathcal{N}\left(#1\right)}}

\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\captionsetup[ruled]{labelsep=period}

\title{Implementing \acl{HMC} for Efficient Bayesian Evolutionary Analysis \\
           \Large\textsc{awc summer scholarship report}}
\author{Arman Bilge \\ \texttt{armanbilge@gmail.com}}
\date{20 April 2015}

\frenchspacing
%\onehalfspacing
\begin{document}

    \maketitle

    \subsection*{Introduction}

    Bayesian evolutionary analysis is centered around sampling from the
        posterior probability distribution of a phylogenetic
        tree~$\mathcal{T}$ and model parameters~$\vec\theta$
        given the molecular sequence data~$\mathcal{D}$~\cite{Bou+14},
        which by Bayes' theorem is
        \begin{equation}
            p\left(\mathcal{T}, \vec\theta \mid \mathcal{D}\right)
                \propto p\left(\mathcal{D} \mid \mathcal{T},\vec\theta\right)
                p\left(\mathcal{T} \mid \vec\theta\right) p\left(\vec\theta\right).
        \end{equation}
    Although direct sampling from this distribution is impossible, sampling
        strategies utilising \ac{MCMC} techniques have been quite
        successful~\cite{RH03,Dru+12,Bou+14}.
    The ideal sampling algorithm maximises the number of pseudo-independent
        samples drawn, called the \ac{ESS}, by minimising the \ac{ACT}.
    \ac{MCMC}

    \ac{HMC}, first described as hybrid Monte Carlo by \textcite{Dua+87},

    \subsection*{Methods}

    Let $\pi\left(\vec{q}\right)$ be the target probability density.
    We can consider a particle in our state space with position given
        by~$\vec{q}$ and momentum by $\vec{p}$.
    Then the Hamiltonian for our system is
        \begin{equation}
            H\left(\vec{q},\vec{p}\right)
            = U\left(\vec{q}\right) + K\left(\vec{p}\right)
        \end{equation}
        with the potential energy
        $U\left(\vec{q}\right) = -\log{\pi\left(\vec{q}\right)}$ and kinetic
        energy $K\left(\vec{p}\right) = \frac{1}{2} \vec{p}^T \mat{M} \vec{p}$,
        where $\mat{M}$ is a positive-definite matrix that we interpret as
        the particle's mass.
    The system's dynamics are described by Hamilton's equations,
        \begin{align}
            \frac{\dd \vec{q}}{\dd t} &= \frac{\partial H}{\partial \vec{p}} \\
            \frac{\dd \vec{p}}{\dd t} &= -\frac{\partial H}{\partial \vec{q}},
        \end{align}
        which are integrated to find the state at a particular time.
    Typically this integration is achieved numerically using the leapfrog
        method~\cite{Nea11}.
    I use $\op{L}\left\{\vec{q},\vec{p}\right\}$ to denote an operator that
        maps the state at time~$t$ to the state at time~$t + \epsilon L$, as
        approximated by $L$~leapfrog steps with stepsize~$\epsilon$.
    Additionally, the momentum flip operator gives
        $\op{F}\left\{\vec{q},\vec{p}\right\}
         = \left\{\vec{q},-\vec{p}\right\}$.
    With the $\op{L}$ and $\op{F}$ operators we can reach a discrete set of
        states along an energy contour (subject only to errors in numerical
        integration).
    To guarantee ergodicity of the Markov chain, the momentum is randomised
        after each iteration of the algorithm by
        \begin{equation}
            \op{R}\left\{\vec{q},\vec{p}\right\} =
            \left\{\vec{q}, \sqrt{1-\alpha}\vec{p} + \sqrt{\alpha}\vec{n}\right\},
            \vec{n} \sim \norm{\vec{0}, \mat{M}^{-1}},
        \end{equation}
        where $\norm{\vec{\mu},\mat{\Sigma}}$ is the multivariate
        normal distribution with mean~$\vec{\mu}$ and covariance~$\mat{\Sigma}$.
    When the parameter $\alpha < 1$ the momentum is only partially refreshed,
        supressing random walk behaviour.
    Using these operators, the \ac{HMC} algorithm is concisely described by
        making $M$ calls to the \textsc{HamiltonUpdate} function given in
        Algorithm~1.
    \begin{algorithm}
        \caption{A single iteration of the \acl{HMC} algorithm that uses
                 Hamiltonian dynamics to make the proposal and the Metropolis
                 criterion to accept or reject it.}
        \begin{algorithmic}[1]
        \Function {HamiltonUpdate}{$\left\{\vec{q},\vec{p}\right\}$}
            \State $\left\{\vec{q}^\prime, \vec{p}^\prime\right\}
                \leftarrow \op{F}\op{L}\left\{\vec{q},\vec{p}\right\}$
            \State $a \leftarrow \min\left(1,
                \exp\left(
                    H\left(\vec{q}, \vec{p}\right) - H\left(\vec{q}^\prime,
                        \vec{p}^\prime\right)\right)\right)$
            \State $\left\{\vec{q},\vec{p}\right\} \leftarrow
                \begin{cases}
                    \left\{\vec{q}^\prime, \vec{p}^\prime\right\}
                        & \text{with probability } a \\
                    \left\{\vec{q},\vec{p}\right\}
                        & \text{with probability } 1 - a
                \end{cases}$
            \State $\left\{\vec{q},\vec{p}\right\} \leftarrow
                        \op{R}\op{F}\left\{\vec{q},\vec{p}\right\}$
            \State \Return $\left\{\vec{q},\vec{p}\right\}$
        \EndFunction
        \end{algorithmic}
    \end{algorithm}

    I implemented the described \ac{HMC} algorithm in a fork of
        BEAST~v1.8~\cite{Dru+12}.
    In particular, I represented the \textsc{HamiltonUpdate} function as a
        standard \ac{MCMC} operator whose Hastings ratio is the exponential of
        the difference between the current and proposed kinetic energy.
    To enable evaluation of the gradient of the potential energy, I added a
        method \texttt{differentiate()} to the core \texttt{Likelihood}
        interface and implemented it for the basic probability distributions,
        coalescent and birth-death tree priors, and the Felsenstein tree
        likelihood~\cite{Fel81}.
    The tree likelihood implementation is optimised and makes use of the
        high-performance library BEAGLE~\cite{Ayr+12}.
    The source code for my fork is freely available under version 3 of the GNU
        General Public
        Licence\footnote{\url{https://github.com/armanbilge/B3/tree/hamilton}}.

    To evaluate the performance of \ac{HMC} relative to \ac{MCMC}I simulated
        ten 32-taxa and ten 64-taxa datasets under the constant size coalescent
        and HKY substitution model with a strict molecular clock ($n = 2048$,
        $\theta = 0.5$, $\mu = 0.15$, $\kappa = 8.0$, $\vec{\pi} = \left[0.35,
        0.30, 0.20, 0.15\right]$).
    I fixed the tree topology and model parameters to the truth and performed
        inference on the branch lengths using both \ac{MCMC} and \ac{HMC}.
    The \ac{MCMC} operators and $\epsilon$ for \ac{HMC} were auto-optimised
        during the analysis and nine different combinations were tried for
        $\left(L, \alpha\right) \in \left\{4, 8, 16\right\} \times \left\{0.33, 0.66, 1.0\right\}$.
    For optimal performance, the mass matrix $\mat{M}$ was set to the inverse
        of the covariance matrix for each dataset~\cite{Nea11}.
    I measured the performance of each algorithm as the \ac{ESS}

    \singlespacing

    \subsection*{Results and Discussion}

    \begin{figure}
        \centering
        \begin{subfigure}{0.45\textwidth}
            \centering
            \includegraphics[scale=0.8]{boxplot.pdf}
            \caption{The efficiency of \ac{HMC} relative to \ac{MCMC}
                     for ten simulated datasets.}
        \end{subfigure}
        \begin{subfigure}{0.45\textwidth}
            \centering
            \includegraphics[scale=0.8]{heatmap.pdf}
            \caption{The efficiency of \ac{HMC} relative to \ac{MCMC}
                         for various values of $L$ and $\alpha$
                         for a particular 64-taxa dataset.}
        \end{subfigure}
    \end{figure}

    \printbibliography

\end{document}

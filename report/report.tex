\documentclass{article}
\usepackage[british]{babel}
\usepackage{csquotes}
\usepackage{hyperref}

\usepackage[backend=biber,style=alphabetic]{biblatex}
\addbibresource{\jobname.bib}

\usepackage{acronym}
\acrodef{ACT}{auto-correlation time}
\acrodef{ESS}{effective sample size}
\acrodef{HMC}{Hamiltonian Monte Carlo}
\acrodef{MCMC}{Markov chain Monte Carlo}
\acrodef{ODE}{ordinary differential equation}

\usepackage{mathtools}
\newcommand{\dd}{\, \mathrm{d}}
\renewcommand{\vec}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\mat}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\op}[1]{\ensuremath{\mathbf{#1}}}

\usepackage{algorithm}
\usepackage{algpseudocode}

\title{Implementing \acl{HMC} for Efficient Bayesian Evolutionary Analysis \\
           \Large\textsc{awc summer scholarship report}}
\author{Arman Bilge \\ \texttt{armanbilge@gmail.com}}
\date{27th March 2015}

\frenchspacing
\begin{document}

    \maketitle

    \subsection*{Introduction}

    Bayesian evolutionary analysis is centered around sampling from the
        posterior probability distribution of a phylogenetic tree~$T$ given the
        molecular sequence
        data~$D$~\cite{Bou+14}, which is defined by
        \begin{equation}
            p\left(T \mid D\right)
                \propto \int_\theta p\left(D \mid T,\theta\right)
                p\left(T \mid \theta\right) p\left(\theta\right) \dd\theta
        \end{equation}
        where $\theta$ is a vector of nuisance parameters for the model.
    Although there is no closed-form expression for this integral, sampling
        strategies utilising Monte Carlo techniques have been quite
        successful~\cite{RH03,Dru+12,Bou+14}.

    The ideal sampling algorithm maximises the number of pseudo-independent
        samples, called the \ac{ESS}, present in all of the samples drawn by
        minimising the \ac{ACT}.

    As the amount of molecular data grows, as does the complexity of the models
        used to analyse them, the large dimensionality of the statespace makes
        evaluating this integral computationally intractable.

    \ac{HMC}, first described as hybrid Monte Carlo by \textcite{Dua+87},

    \subsection*{Methods}

    Let $\pi\left(\vec{q}\right)$ be the target probability density.
    We augment the state space with momentum $\vec{p}$ and define the
        Hamiltonian for our system as
        \begin{equation}
            H\left(\vec{q},\vec{p}\right)
            = U\left(\vec{q}\right) + K\left(\vec{p}\right)
        \end{equation}
        with the potential energy
        $U\left(\vec{q}\right) = -\log{\pi\left(\vec{q}\right)}$ and kinetic
        energy $K\left(\vec{p}\right) = \frac{1}{2} \vec{p}^T \mat{M} \vec{p}$,
        where $\mat{M}$ is the mass matrix for the particle (often simply the
        identity matrix).
    The system's dynamics are described by Hamilton's equations,
        \begin{align}
            \frac{\dd \vec{q}}{\dd t} &= \frac{\partial H}{\partial \vec{p}} \\
            \frac{\dd \vec{p}}{\dd t} &= -\frac{\partial H}{\partial \vec{q}},
        \end{align}
        which are integrated to find the state at a particular time.
    Typically this integration is achieved numerically using the leapfrog
        method, a symplectic integrator~\cite{Nea11}.
    I use $\op{L}\left\{\vec{q},\vec{p}\right\}$ to denote an operator that
        maps the state at time~$t$ to the state at time~$t + \epsilon L$, as
        approximated by $L$~leapfrog steps with stepsize~$\epsilon$.
    Additionally, the momentum flip operator
        $\op{F}\left\{\vec{q},\vec{p}\right\} = \left\{\vec{q},-\vec{p}\right\}$.

    \begin{algorithm}
        \caption{A \ac{MCMC} operator that simulates Hamiltonian dynamics to
                 make proposals.}
        \begin{algorithmic}[1]
        \Function {HamiltonUpdate}{$\left\{\vec{q},\vec{p}\right\}$}
            \State $\left\{\vec{q}^\prime, \vec{p}^\prime\right\}
                \leftarrow \op{F}\op{L}\left\{\vec{q},\vec{p}\right\}$
            \State $a \leftarrow \min\left(1,
                \exp\left(
                    H\left(\vec{q}, \vec{p}\right) - H\left(\vec{q}^\prime,
                        \vec{p}^\prime\right)\right)\right)$
            \State $\left\{\vec{q},\vec{p}\right\} \leftarrow
                \begin{cases}
                    \left\{\vec{q}^\prime, \vec{p}^\prime\right\}
                        & \text{with probability } a \\
                    \left\{\vec{q},\vec{p}\right\}
                        & \text{with probability } 1 - a
                \end{cases}$
            \State $\left\{\vec{q},\vec{p}\right\} \leftarrow
                        \op{R}\op{F}\left\{\vec{q},\vec{p}\right\}$
            \State \Return $\left\{\vec{q},\vec{p}\right\}$
        \EndFunction
        \end{algorithmic}
    \end{algorithm}

    I implemented the described \ac{HMC} algorithm in a fork of
        BEAST~v1.8~\cite{Dru+12}.

    To enable evaluation of the gradient, I introduced a
        \texttt{Differentiable} interface that defines a method
        \texttt{differentiate()} that returns the gradient with respect to a
        variable.
    The existing \texttt{Likelihood} interface was then modified to extend
        \texttt{Differentiable}.
    The source code for my fork is freely available under version 3 of the GNU
        General Public License at
        \url{https://github.com/armanbilge/BEAST3/tree/hamilton}.

    \subsection*{Results and Discussion}

    \printbibliography

\end{document}

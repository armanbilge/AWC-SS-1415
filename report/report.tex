\documentclass{article}
\usepackage[british]{babel}
\usepackage{csquotes}
\usepackage{hyperref}

\usepackage[backend=biber,style=alphabetic]{biblatex}
\addbibresource{\jobname.bib}

\usepackage{acronym}
\acrodef{ESS}{effective sample size}
\acrodef{HMC}{Hamiltonian Monte Carlo}
\acrodef{MCMC}{Markov chain Monte Carlo}
\acrodef{ODE}{ordinary differential equation}

\usepackage{mathtools}
\newcommand{\dd}{\; \mathrm{d}}
\renewcommand{\vec}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\mat}[1]{\ensuremath{\mathbf{#1}}}

\title{Implementing \acl{HMC} for Efficient Bayesian Evolutionary Analysis \\
           \Large\textsc{awc summer scholarship report}}
\author{Arman Bilge \\ \texttt{armanbilge@gmail.com}}
\date{27th March 2015}

\frenchspacing
\begin{document}

    \maketitle

    \subsection*{Introduction}

    Bayesian evolutionary analysis is centered around the posterior probability
        distribution of a phylogenetic tree~$T$ given the molecular sequence
        data~$D$~\cite{Bou+14}, which is defined by
        \begin{equation}
            p\left(T \mid D\right)
                \propto \int_\theta p\left(D \mid T,\theta\right)
                p\left(T \mid \theta\right) p\left(\theta\right) \dd\theta
        \end{equation}
        where $\theta$ is a vector of nuisance parameters for the model.
    Although there is no closed-form expression for this integral, sampling
        strategies utilising Monte Carlo techniques,  have been quite
        successful~\cite{RH03,Dru+12,Bou+14}.

    As the amount of molecular data grows, as does the complexity of the models
        used to analyse them, the large dimensionality of the statespace makes
        evaluating this integral computationally intractable.

    \ac{HMC}, first described as hybrid Monte Carlo by \textcite{Dua+87},

    \subsection*{Methods}

    Let $\pi\left(\vec{q}\right)$ be the target probability density.
    We augment the state space with momentum $\vec{p}$ and define the
        Hamiltonian for our system as
        \begin{equation}
            H\left(\vec{q},\vec{p}\right)
            = U\left(\vec{q}\right) + K\left(\vec{p}\right)
        \end{equation}
        with the potential energy
        $U\left(\vec{q}\right) = -\log{\pi\left(\vec{q}\right)}$ and kinetic
        energy $K\left(\vec{p}\right) = \frac{1}{2} \vec{p}^T \mat{M} \vec{p}$,
        where $\mat{M}$ is the mass matrix for the particle (often simply the
        identity matrix).
    The system's dynamics are described by Hamilton's equations,
        \begin{equation}
            \frac{\dd \vec{q}}{\dd t} = \frac{\partial H}{\partial \vec{p}} \\
            \frac{\dd \vec{p}}{\dd t} = - \frac{\partial H}{\partial \vec{q}}
        \end{equation}
    To describe the \ac{HMC} algorithm, I introduce three operators:

    I implemented the described \ac{HMC} algorithm in a fork of
        BEAST~1~\cite{Dru+12}.

    To enable evaluation of the gradient, I introduced a
        \texttt{Differentiable} interface that defines a method
        \texttt{differentiate()} that returns the gradient with respect to a
        variable.
    The existing \texttt{Likelihood} interface was then modified to extend
        \texttt{Differentiable}.
    The source code for my fork is freely available under version 3 of the GNU
        General Public License at
        \url{https://github.com/armanbilge/BEAST3/tree/hamilton}.

    \subsection*{Results and Discussion}

    \printbibliography

\end{document}

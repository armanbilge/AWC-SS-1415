\documentclass{article}

\usepackage[margin=3cm]{geometry}
\usepackage{setspace}

\usepackage[british]{babel}
\usepackage{csquotes}
\usepackage{hyperref}

\usepackage{titling}
\setlength{\droptitle}{-2cm}

\usepackage[backend=biber,style=alphabetic]{biblatex}
\addbibresource{\jobname.bib}

\usepackage{acronym}
\acrodef{ACT}{auto-correlation time}
\acrodef{ESS}{effective sample size}
\acrodef{HMC}{Hamiltonian Monte Carlo}
\acrodef{MCMC}{Markov chain Monte Carlo}
\acrodef{ODE}{ordinary differential equation}

\usepackage{mathtools}
\newcommand{\dd}{\, \mathrm{d}}
\renewcommand{\vec}[1]{\ensuremath{\boldsymbol{\mathbf{#1}}}}
\newcommand{\mat}[1]{\ensuremath{\boldsymbol{\mathbf{#1}}}}
\newcommand{\op}[1]{\ensuremath{\boldsymbol{\mathbf{#1}}}}
\newcommand{\norm}[1]{\ensuremath{\mathcal{N}\left(#1\right)}}

\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\captionsetup[ruled]{labelsep=period}

\title{Implementing \acl{HMC} for Efficient Bayesian Evolutionary Analysis \\
           \Large\textsc{awc summer scholarship report}}
\author{Arman Bilge \\ \texttt{armanbilge@gmail.com}}
\date{17 April 2015}

\frenchspacing
\onehalfspacing
\begin{document}

    \maketitle

    \subsection*{Introduction}

    Bayesian evolutionary analysis is centered around sampling from the
        posterior probability distribution of a phylogenetic
        tree~$\mathcal{T}$ and model parameters~$\vec\theta$
        given the molecular sequence data~$\mathcal{D}$~\cite{Bou+14},
        which by Bayes' theorem is
        \begin{equation}
            p\left(\mathcal{T}, \vec\theta \mid \mathcal{D}\right)
                \propto p\left(\mathcal{D} \mid \mathcal{T},\vec\theta\right)
                p\left(\mathcal{T} \mid \vec\theta\right) p\left(\vec\theta\right).
        \end{equation}
    Although direct sampling from this distribution is impossible, sampling
        strategies utilising \ac{MCMC} techniques have been quite
        successful~\cite{RH03,Dru+12,Bou+14}.

    Constructing a Markov chain that

    Thus, a proposal distribution that is the target distribution

    The ideal sampling algorithm maximises the number of pseudo-independent
        samples, called the \ac{ESS}, by minimising the \ac{ACT}.
    For multidimensional distributions, the \ac{ESS} of the slowest-mixing
        dimension is representative of the overall \ac{ESS}~\cite{Tho10}.

    Today, the amount of molecular data available grows rapidly, as does the
        complexity of the models used to analyse them.

    \ac{HMC}, first described as hybrid Monte Carlo by \textcite{Dua+87},

    \subsection*{Methods}

    Let $\pi\left(\vec{q}\right)$ be the target probability density.
    We can consider a particle in our state space with position given
        by~$\vec{q}$ and momentum by $\vec{p}$.
    Then the Hamiltonian for our system is
        \begin{equation}
            H\left(\vec{q},\vec{p}\right)
            = U\left(\vec{q}\right) + K\left(\vec{p}\right)
        \end{equation}
        with the potential energy
        $U\left(\vec{q}\right) = -\log{\pi\left(\vec{q}\right)}$ and kinetic
        energy $K\left(\vec{p}\right) = \frac{1}{2} \vec{p}^T \mat{M} \vec{p}$,
        where $\mat{M}$ is a positive-definite matrix that we interpret as
        the particle's mass.
    The system's dynamics are described by Hamilton's equations,
        \begin{align}
            \frac{\dd \vec{q}}{\dd t} &= \frac{\partial H}{\partial \vec{p}} \\
            \frac{\dd \vec{p}}{\dd t} &= -\frac{\partial H}{\partial \vec{q}},
        \end{align}
        which are integrated to find the state at a particular time.
    Typically this integration is achieved numerically using the leapfrog
        method~\cite{Nea11}.
    I use $\op{L}\left\{\vec{q},\vec{p}\right\}$ to denote an operator that
        maps the state at time~$t$ to the state at time~$t + \epsilon L$, as
        approximated by $L$~leapfrog steps with stepsize~$\epsilon$.
    Additionally, the momentum flip operator gives
        $\op{F}\left\{\vec{q},\vec{p}\right\}
         = \left\{\vec{q},-\vec{p}\right\}$.
    With the $\op{L}$ and $\op{F}$ operators we can reach a discrete set of
        states along an energy contour (subject only to errors in numerical
        integration).
    To guarantee ergodicity of the Markov chain, the momentum is randomised
        after each iteration of the algorithm by
        \begin{equation}
            \op{R}\left\{\vec{q},\vec{p}\right\} =
            \left\{\vec{q}, \sqrt{1-\alpha}\vec{p} + \sqrt{\alpha}\vec{n}\right\},
            \vec{n} \sim \norm{\vec{0}, \mat{M}^{-1}},
        \end{equation}
        where $\norm{\vec{\mu},\mat{\Sigma}}$ is the multivariate
        normal distribution with mean~$\vec{\mu}$ and covariance~$\mat{\Sigma}$.
    Using these operators,
    \begin{algorithm}
        \caption{A single iteration of the \acl{HMC} algorithm that uses
                 Hamiltonian dynamics to make the proposal and the Metropolis
                 criterion to accept or reject it.}
        \begin{algorithmic}[1]
        \Function {HamiltonUpdate}{$\left\{\vec{q},\vec{p}\right\}$}
            \State $\left\{\vec{q}^\prime, \vec{p}^\prime\right\}
                \leftarrow \op{F}\op{L}\left\{\vec{q},\vec{p}\right\}$
            \State $a \leftarrow \min\left(1,
                \exp\left(
                    H\left(\vec{q}, \vec{p}\right) - H\left(\vec{q}^\prime,
                        \vec{p}^\prime\right)\right)\right)$
            \State $\left\{\vec{q},\vec{p}\right\} \leftarrow
                \begin{cases}
                    \left\{\vec{q}^\prime, \vec{p}^\prime\right\}
                        & \text{with probability } a \\
                    \left\{\vec{q},\vec{p}\right\}
                        & \text{with probability } 1 - a
                \end{cases}$
            \State $\left\{\vec{q},\vec{p}\right\} \leftarrow
                        \op{R}\op{F}\left\{\vec{q},\vec{p}\right\}$
            \State \Return $\left\{\vec{q},\vec{p}\right\}$
        \EndFunction
        \end{algorithmic}
    \end{algorithm}

    I implemented the described \ac{HMC} algorithm in a fork of
        BEAST~v1.8~\cite{Dru+12}.

    To enable evaluation of the gradient of the potential energy, I added a
        method \texttt{differentiate()} to the core \texttt{Likelihood}
        interface and implemented it for the basic probability distributions,
        coalescent and birth-death tree priors, and the Felsenstein tree
        likelihood~\cite{Fel81}.
    The tree likelihood implementation is optimised and makes use of the
        high-performance library BEAGLE~\cite{Ayr+12}.
    The source code for my fork is freely available under version 3 of the GNU
        General Public
        Licence\footnote{\url{https://github.com/armanbilge/B3/tree/hamilton}}.

    I simulated twenty random datasets under a constant size coalescent

    \singlespacing

    \subsection*{Results and Discussion}

    \begin{figure}
        \centering
        \begin{subfigure}{0.45\textwidth}
            \centering
            \includegraphics[scale=0.8]{boxplot.pdf}
            \caption{The efficiency of \ac{HMC} relative to \ac{MCMC}
                     for ten simulated datasets.}
        \end{subfigure}
        \begin{subfigure}{0.45\textwidth}
            \centering
            \includegraphics[scale=0.8]{heatmap.pdf}
            \caption{The efficiency of \ac{HMC} relative to \ac{MCMC}
                         for various values of $L$ and $\alpha$
                         for a particular 64-taxa dataset.}
        \end{subfigure}
    \end{figure}

    \printbibliography

\end{document}
